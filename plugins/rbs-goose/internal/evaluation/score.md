Follow these instructions:

```
# Score evaluation results using Claude's analysis
# Parameters: results (array of test case results)

results = params[:results]

puts("\n" + "=" * 80)
puts("EVALUATION SUMMARY")
puts("=" * 80)

# Calculate aggregate statistics
total_tests = results.length
successful_tests = results.count { |r| r[:success] }
failed_tests = total_tests - successful_tests
total_iterations = results.sum { |r| r[:iterations] }
avg_iterations = total_tests > 0 ? (total_iterations.to_f / total_tests).round(2) : 0
total_time = results.sum { |r| r[:execution_time] }

puts("\nAggregate Statistics:")
puts("  Total test cases: #{total_tests}")
puts("  Successful: #{successful_tests} (#{(successful_tests.to_f / total_tests * 100).round(1)}%)")
puts("  Failed: #{failed_tests}")
puts("  Average iterations: #{avg_iterations}")
puts("  Total execution time: #{total_time.round(2)}s")

puts("\n" + "-" * 80)
puts("DETAILED RESULTS")
puts("-" * 80)

# Score each test case
scored_results = []

results.each do |result|
  puts("\nTest Case: #{result[:name]}")
  puts("  Status: #{result[:success] ? 'PASS' : 'FAIL'}")

  if result[:error_message]
    puts("  Error: #{result[:error_message]}")
    score = {
      test_case: result[:name],
      overall_score: 0,
      success: false,
      comment: "Failed with error: #{result[:error_message]}"
    }
    scored_results << score
    next
  end

  puts("  Iterations: #{result[:iterations]}")
  puts("  Execution time: #{result[:execution_time].round(2)}s")
  puts("  Errors: #{result[:error_count_before]} → #{result[:error_count_after]}")
  puts("  Generated files: #{result[:generated_files].length}")

  # Read generated files for analysis
  evaluation_dir = './tmp/rbs_goose_evaluation'
  test_case_path = File.join(evaluation_dir, result[:name])

  generated_content = []
  result[:generated_files].each do |file_path|
    full_path = File.join(test_case_path, file_path)
    if File.exists?(full_path)
      content = File.read(full_path)
      generated_content << {
        path: file_path,
        content: content
      }
    end
  end

  # Prepare evaluation prompt for Claude
  evaluation_prompt = <<~PROMPT
    Evaluate the quality of RBS type annotations generated by rbs-goose for this test case.

    Test Case: #{result[:name]}
    Success: #{result[:success]}
    Iterations: #{result[:iterations]}
    Error Reduction: #{result[:error_count_before]} → #{result[:error_count_after]}

    Generated Files (#{generated_content.length}):
    #{generated_content.map { |f| "- #{f[:path]}" }.join("\n")}

    Sample Content:
    #{generated_content.first(3).map { |f| "=== #{f[:path]} ===\n#{f[:content][0..500]}\n" }.join("\n")}

    Please evaluate on the following criteria (score 0-10 for each):
    1. Correctness: Are the type annotations accurate and meaningful?
    2. Completeness: Are all necessary types defined?
    3. Quality: Are the types well-structured and idiomatic?
    4. Efficiency: Did it converge quickly without excessive iterations?

    Provide scores and a brief comment (2-3 sentences).
    Format your response as:
    Correctness: X/10
    Completeness: X/10
    Quality: X/10
    Efficiency: X/10
    Overall: X/10
    Comment: <your comment>
  PROMPT

  puts("\n  Analyzing generated types with Claude...")

  # This is a pseudo-function that Claude will interpret
  # Claude will actually read the files and provide evaluation
  claude_response = ask_claude(evaluation_prompt)

  # Parse Claude's response
  score = {
    test_case: result[:name],
    success: result[:success],
    iterations: result[:iterations],
    error_reduction: "#{result[:error_count_before]} → #{result[:error_count_after]}",
    claude_evaluation: claude_response
  }

  # Extract scores from response (simplified parsing)
  if claude_response =~ /Overall:\s*(\d+)\/10/
    score[:overall_score] = $1.to_i
  end

  if claude_response =~ /Comment:\s*(.+?)(?:\n|$)/m
    score[:comment] = $1.strip
  end

  puts("  Overall Score: #{score[:overall_score]}/10")
  puts("  Comment: #{score[:comment]}")

  scored_results << score
end

# Final summary
puts("\n" + "=" * 80)
puts("FINAL SCORING SUMMARY")
puts("=" * 80)

avg_score = scored_results.sum { |s| s[:overall_score].to_i } / scored_results.length.to_f
puts("\nAverage Score: #{avg_score.round(1)}/10")

puts("\nScore Distribution:")
scored_results.group_by { |s| s[:overall_score] }.sort.each do |score, cases|
  puts("  #{score}/10: #{cases.map { |c| c[:test_case] }.join(', ')}")
end

puts("\nTop Performers:")
scored_results.sort_by { |s| -s[:overall_score].to_i }.first(3).each do |s|
  puts("  #{s[:test_case]}: #{s[:overall_score]}/10 - #{s[:comment]}")
end

if failed_tests > 0
  puts("\nFailed Tests:")
  scored_results.select { |s| !s[:success] }.each do |s|
    puts("  #{s[:test_case]}: #{s[:comment]}")
  end
end

# Save results to file
results_file = './rbs_goose_evaluation_results.json'
File.write(results_file, JSON.pretty_generate({
  summary: {
    total_tests: total_tests,
    successful_tests: successful_tests,
    failed_tests: failed_tests,
    average_iterations: avg_iterations,
    total_time: total_time,
    average_score: avg_score
  },
  detailed_results: scored_results,
  timestamp: Time.now.iso8601
}))

puts("\nResults saved to: #{results_file}")
```
